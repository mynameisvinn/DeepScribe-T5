{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from en_core_web_sm==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.42.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (45.2.0.post20200210)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=3bc3726ba05964566a94934735a413aca9298df302e62d4bd904b65a243f0122\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ghan_w12/wheels/10/6f/a6/ddd8204ceecdedddea923f8514e13afb0c1f0f556d2c9c3da0\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: spacy[cuda92] in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2.3.4)\n",
      "Collecting spacy[cuda92]\n",
      "  Downloading spacy-3.0.3-cp36-cp36m-manylinux2014_x86_64.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 25.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (2.11.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (4.42.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (20.8)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (45.2.0.post20200210)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (0.7.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (3.7.4.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from spacy[cuda92]) (2.0.4)\n",
      "Collecting catalogue<2.1.0,>=2.0.1\n",
      "  Downloading catalogue-2.0.1-py3-none-any.whl (9.6 kB)\n",
      "Collecting cupy-cuda92<9.0.0,>=5.0.0b4\n",
      "  Downloading cupy_cuda92-8.4.0-cp36-cp36m-manylinux1_x86_64.whl (355.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 354.1 MB 58.7 MB/s eta 0:00:01\u001b[K     |████████████████████████████████| 355.1 MB 33 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting fastrlock>=0.3\n",
      "  Downloading fastrlock-0.5-cp36-cp36m-manylinux1_x86_64.whl (31 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata>=0.20->spacy[cuda92]) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging>=20.0->spacy[cuda92]) (2.4.7)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "  Downloading pydantic-1.7.3-cp36-cp36m-manylinux2014_x86_64.whl (9.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.2 MB 41.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses>=0.6\n",
      "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy[cuda92]) (1.26.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
      "  Downloading spacy_legacy-3.0.1-py2.py3-none-any.whl (7.0 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.0\n",
      "  Downloading srsly-2.4.0-cp36-cp36m-manylinux2014_x86_64.whl (456 kB)\n",
      "\u001b[K     |████████████████████████████████| 456 kB 58.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.0\n",
      "  Downloading thinc-8.0.1-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 34.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contextvars<3,>=2.4\n",
      "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
      "Collecting immutables>=0.9\n",
      "  Downloading immutables-0.15-cp36-cp36m-manylinux1_x86_64.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 15.2 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
      "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
      "Collecting click<7.2.0,>=7.1.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from jinja2->spacy[cuda92]) (1.1.1)\n",
      "Collecting pathy\n",
      "  Downloading pathy-0.4.0-py3-none-any.whl (36 kB)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
      "\u001b[K     |████████████████████████████████| 113 kB 66.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: contextvars, smart-open\n",
      "  Building wheel for contextvars (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7664 sha256=540cc70ad57b880afbfdef3e20621922145f492510489c48f223914df220c8e9\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/41/11/53/911724983aa48deb94792432e14e518447212dd6c5477d49d3\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107097 sha256=c60b3837f370fb148cb95f7ae6b3e290fc2df6064b37e94d22c13bc5ab92dd7b\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/88/2a/d4/f2e9023989d4d4b3574f268657cb6cd23994665a038803f547\n",
      "Successfully built contextvars smart-open\n",
      "Installing collected packages: immutables, dataclasses, click, catalogue, wasabi, typer, srsly, smart-open, pydantic, contextvars, thinc, spacy-legacy, pathy, fastrlock, spacy, cupy-cuda92\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: Click 7.0\n",
      "    Uninstalling Click-7.0:\n",
      "      Successfully uninstalled Click-7.0\n",
      "  Attempting uninstall: catalogue\n",
      "    Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Attempting uninstall: wasabi\n",
      "    Found existing installation: wasabi 0.8.0\n",
      "    Uninstalling wasabi-0.8.0:\n",
      "      Successfully uninstalled wasabi-0.8.0\n",
      "  Attempting uninstall: srsly\n",
      "    Found existing installation: srsly 1.0.4\n",
      "    Uninstalling srsly-1.0.4:\n",
      "      Successfully uninstalled srsly-1.0.4\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.3\n",
      "    Uninstalling thinc-7.4.3:\n",
      "      Successfully uninstalled thinc-7.4.3\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.3.4\n",
      "    Uninstalling spacy-2.3.4:\n",
      "      Successfully uninstalled spacy-2.3.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 2.3.1 requires spacy<2.4.0,>=2.3.0, but you have spacy 3.0.3 which is incompatible.\u001b[0m\n",
      "Successfully installed catalogue-2.0.1 click-7.1.2 contextvars-2.4 cupy-cuda92-8.4.0 dataclasses-0.8 fastrlock-0.5 immutables-0.15 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.3 spacy-legacy-3.0.1 srsly-2.4.0 thinc-8.0.1 typer-0.3.2 wasabi-0.8.2\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##issue with gpt: seed the sentence (no encoders), hence, T5\n",
    "!pip3 install -r requirements.txt\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!pip3 install -U spacy[cuda92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d80358a221a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAdafactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration,Adafactor\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_df.csv')\n",
    "df=pd.read_csv('../DataSaurCompanion/lsa_filtered_tfidf.csv')\n",
    "len(test_df), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = []\n",
    "for i in range(len(np.array(df['input_text']))):\n",
    "    if np.array(df['input_text'])[i] in np.array(test_df['input_text']):\n",
    "        mask.append(False)\n",
    "    else:\n",
    "        mask.append(True)\n",
    "mask = np.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[mask]\n",
    "train_df.to_csv('train_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../DataSaurCompanion/lsa_filtered_tfidf.csv')\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "#train_df=train_df.iloc[  :35000,:]\n",
    "train_df=df[mask]\n",
    "test_df = df[~mask]\n",
    "print(len(train_df), len(test_df))\n",
    "train_df.to_csv('train_df.csv')\n",
    "test_df.to_csv('test_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv')\n",
    "test_df = pd.read_csv('test_df.csv')\n",
    "training_column = \"cat_conc_sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(train_df[training_column])[11:15], np.array(train_df['input_text'])[11:15] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ars =  np.array(df['input_text'])\n",
    "mas = []\n",
    "for a in ars:\n",
    "    mas.append(len(a))\n",
    "mas = np.array(mas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(mas>400)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(mas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "   dev = torch.device(\"cuda:2\")\n",
    "   print(\"Running on the GPU\")\n",
    "else:\n",
    "   dev = torch.device(\"cpu\")\n",
    "   print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base',\n",
    "                                             return_dict=True)\n",
    "#moving the model to GPU\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('pytorch_model_categories.bin'))\n",
    "#model.load_state_dict(torch.load('pytorch_model_categories_87_precision.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "def progress(loss,value, max=100):\n",
    " return HTML(\"\"\" Batch loss :{loss}\n",
    "      <progress    \n",
    "value='{value}'max='{max}',style='width: 100%'>{value}\n",
    "      </progress>\n",
    "             \"\"\".format(loss=loss,value=value, max=max))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_of_batches=len(test_df)/batch_size\n",
    "num_of_batches = int(num_of_batches)\n",
    "test_inp_batches = []\n",
    "test_label_batches =[]\n",
    "for i in range(num_of_batches):\n",
    "    \n",
    "    new_df=test_df[i*batch_size:i*batch_size+batch_size]\n",
    "    inputbatch=[]\n",
    "    labelbatch=[]\n",
    "    for indx,row in new_df.iterrows():\n",
    "          input = row[training_column]\n",
    "          labels = row['target_text'] \n",
    "          inputbatch.append(input)\n",
    "          labelbatch.append(labels)\n",
    "    test_inputbatch=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=400,return_tensors='pt')[\"input_ids\"]\n",
    "    test_labelbatch=tokenizer.batch_encode_plus(labelbatch,padding=True,max_length=400,return_tensors=\"pt\") [\"input_ids\"]\n",
    "    test_inputbatch=test_inputbatch.to(dev)\n",
    "    test_labelbatch=test_labelbatch.to(dev)\n",
    "    test_inp_batches.append(test_inputbatch)\n",
    "    test_label_batches.append(test_labelbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_inputbatch), len(test_inputbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def generate(texts, model, tokenizer, targets):\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    batch_size=256\n",
    "    num_of_batches=len(texts)*1.0/batch_size\n",
    "    num_of_batches = math.ceil(num_of_batches)\n",
    "    \n",
    "    outputs = []\n",
    "    targs = []\n",
    "    for i in range(num_of_batches):\n",
    "        #print(i, len(outputs), len(targs))\n",
    "        inputbatch = []\n",
    "        for text in texts[i*batch_size:i*batch_size+batch_size]:\n",
    "            input = text  \n",
    "            inputbatch.append(input)\n",
    "        try:\n",
    "            inputbatch=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=400,return_tensors='pt')[\"input_ids\"]\n",
    "            inputbatch=inputbatch.to(dev)\n",
    "            model.to(dev)\n",
    "            model.eval()\n",
    "            outs = model.generate(inputbatch)\n",
    "            outputs.extend(outs)\n",
    "            targs.extend(targets[i*batch_size:i*batch_size+batch_size])\n",
    "        except Exception as e:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "    return [tokenizer.decode(out) for out in outputs], targs\n",
    "\n",
    "def evaluate(inps, targetss, model, tokenizer):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    preds, targets = generate(inps, model, tokenizer, targetss)\n",
    "    for i in range(len(targets)):\n",
    "        target = targets[i]\n",
    "        pred = preds[i]#generate(inp, model, tokenizer)\n",
    "        p, r = rouge_n(target, pred, 1)\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "    return np.mean(precisions), np.mean(recalls)\n",
    "\n",
    "def rouge_n(target, pred, n):\n",
    "    target_list = target.split(' ')\n",
    "    pred_list = pred.split(' ')\n",
    "    n_gram_target = get_n_gram_list(n, target)\n",
    "    n_gram_pred = get_n_gram_list(n, pred)\n",
    "    match = 0\n",
    "    \n",
    "    for i in range(min(len(n_gram_target),len(n_gram_pred))):\n",
    "        if n_gram_target[i] in n_gram_pred:\n",
    "            match += 1\n",
    "    precision = match*1.0/len(n_gram_pred)\n",
    "    recall = match*1.0/len(n_gram_target)\n",
    "    return [precision, recall]\n",
    "import re\n",
    "def get_n_gram_list(n, text):\n",
    "    text = text.replace('/pad>', '')\n",
    "    text = text.replace('</s>', '').strip()\n",
    "    target_list = text.split(' ')\n",
    "    n_gram_target = []\n",
    "    for i in range(len(target_list)-n+1):\n",
    "        lis = [ re.sub(r'[^\\w\\s]', '', w.strip().lower()) for w in target_list[i:i+n]]\n",
    "        n_gram_target.append(' '.join(lis))\n",
    "    return n_gram_target\n",
    "import nltk, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "stop_words = list(set(stopwords.words(\"english\")))\n",
    "stop_words += list(string.punctuation)\n",
    "stop_words += ['__', '___']\n",
    "def getlsa(texts):\n",
    "    tokenizer = RegexpTokenizer(r'\\b\\w{3,}\\b')\n",
    "    tfidf = TfidfVectorizer(lowercase=True, \n",
    "                            stop_words=stop_words, \n",
    "                            tokenizer=tokenizer.tokenize, \n",
    "#                             max_df=0.2,\n",
    "#                             min_df=0.02\n",
    "                           )\n",
    "    tfidf_train_sparse = tfidf.fit_transform(texts)\n",
    "    tfidf_train_df = pd.DataFrame(tfidf_train_sparse.toarray(), \n",
    "                            columns=tfidf.get_feature_names())\n",
    "    lsa_obj = TruncatedSVD(n_components=10, n_iter=1000, random_state=42)\n",
    "    tfidf_lsa_data = lsa_obj.fit_transform(tfidf_train_df)\n",
    "    return tfidf_lsa_data\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "def sim(v1, v2):\n",
    "    return np.dot(v1,v2)\n",
    "def lsa_evaluate(inps, targetss, model, tokenizer, concepts):\n",
    "    preds, targets = generate(inps, model, tokenizer, targetss)\n",
    "    real_score = 0\n",
    "    for pred in preds:\n",
    "        score = 0\n",
    "        for concept in concepts:\n",
    "            score += sim(concept, pred)\n",
    "        real_score += score*1.0/len(concepts)\n",
    "    \n",
    "    return real_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adafactor(model.parameters(),lr=1e-4,\n",
    "                      eps=(1e-30, 1e-3),\n",
    "                      clip_threshold=1.0,\n",
    "                      decay_rate=-0.8,\n",
    "                      beta1=None,\n",
    "                      weight_decay=0.0,\n",
    "                      relative_step=False,\n",
    "                      scale_parameter=False,\n",
    "                      warmup_init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check examples after every epoch\n",
    "import gc\n",
    "import traceback\n",
    "num_of_epochs = 2\n",
    "batch_size=16\n",
    "num_of_batches=len(train_df)/batch_size\n",
    "num_of_batches = int(num_of_batches)\n",
    "\n",
    "#Sets the module in training mode\n",
    "model.train()\n",
    "\n",
    "loss_per_10_steps=[]\n",
    "for epoch in range(1,num_of_epochs+1):\n",
    "    print('Running epoch: {}'.format(epoch))\n",
    "    print('{precision, recall} = ', evaluate(np.array(test_df[training_column]), np.array(test_df['target_text']), model, tokenizer))\n",
    "    running_loss=0\n",
    "\n",
    "    out = display(progress(1, num_of_batches+1), display_id=True)\n",
    "    \n",
    "    for i in range(num_of_batches):\n",
    "        if i%500 == 0:\n",
    "            print(i, num_of_batches)\n",
    "        new_df=train_df[i*batch_size:i*batch_size+batch_size]\n",
    "        inputbatch=[]\n",
    "        labelbatch=[]\n",
    "        for indx,row in new_df.iterrows():\n",
    "            input = row[training_column]\n",
    "            labels = row['target_text']\n",
    "            inputbatch.append(input)\n",
    "            labelbatch.append(labels)\n",
    "        inputbatch=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=400,return_tensors='pt')[\"input_ids\"]\n",
    "        labelbatch=tokenizer.batch_encode_plus(labelbatch,padding=True,max_length=400,return_tensors=\"pt\") [\"input_ids\"]\n",
    "        inputbatch=inputbatch.to(dev)\n",
    "        labelbatch=labelbatch.to(dev)\n",
    "\n",
    "        # clear out the gradients of all Variables \n",
    "        optimizer.zero_grad()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Forward propogation\n",
    "        try:\n",
    "            outputs = model(input_ids=inputbatch, labels=labelbatch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss_num=loss.item()\n",
    "            logits = outputs.logits\n",
    "            running_loss+=loss_num\n",
    "            if i%10 ==0:      \n",
    "                loss_per_10_steps.append(loss_num)\n",
    "            out.update(progress(loss_num,i, num_of_batches+1))\n",
    "\n",
    "            # calculating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #updating the params\n",
    "            optimizer.step()\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            #traceback.print_exc()\n",
    "            torch.save(model.state_dict(),'pytorch_model_categories.bin')\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "    running_loss=running_loss/int(num_of_batches)\n",
    "    torch.save(model.state_dict(),'pytorch_model_categories.bin')\n",
    "    print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
    "    ix = 0\n",
    "    test_output_loss = 0\n",
    "    while ix < len(test_inp_batches):\n",
    "        if ix%200==0:\n",
    "            print(ix, len(test_inp_batches))\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            test_output_loss += model(input_ids=test_inp_batches[ix], labels=test_label_batches[ix]).loss.item()\n",
    "            ix += 1\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            ix += 1\n",
    "        \n",
    "    print('Validation Loss:', test_output_loss/int(num_of_batches))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision: when we predict the word and it's present in target set\n",
    "#Recall: When word is in target set and we predict it as well\n",
    "test_df.to_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'42_recall_51_lsa.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{precision, recall} = ', evaluate(np.array(train_df[training_column]), np.array(train_df['target_text']), model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = df\n",
    "print('{precision, recall} = ', evaluate(np.array(test_df[training_column]), np.array(test_df['target_text']), model, tokenizer)), #0.0468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feed like this ? Incorporate medical concepts in the following way ?\n",
    "term 1 | term 2 |\n",
    "term 1 <concept1> | term 2 <concept2>\n",
    "\n",
    "instead of using terms, just use concepts ?\n",
    "\n",
    "train the model specifically for each category (e.g.: symptom, medications, family history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(test_df['input_text'])[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[15:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "ix = 500\n",
    "print(np.array(test_df['target_text'])[ix])\n",
    "print(np.array(test_df[training_column])[ix])\n",
    "generate(np.array(test_df[training_column])[ix:ix+1], model, tokenizer, np.array(test_df['target_text'])[ix:ix+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rouge modified evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = generate(np.array(test_df['input_text']), model, tokenizer, np.array(test_df['target_text']))\n",
    "pred_lsas = getlsa(preds)\n",
    "target_lsas = getlsa(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lsas.shape, len(preds), target_lsas.shape, len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9\n",
    "print(np.array(test_df['input_text'])[k], np.array(test_df['target_text'])[k], preds[k])\n",
    "sim(pred_lsas[k], target_lsas[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su = 0\n",
    "for k in range(len(preds)):\n",
    "    su += sim(pred_lsas[k], target_lsas[k])\n",
    "su, su/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "preds, targets = generate(np.array(test_df['input_text']), model, tokenizer, np.array(test_df['target_text']))\n",
    "print('Predictions done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(targets)):\n",
    "    target = targets[i]\n",
    "    pred = preds[i]#generate(inp, model, tokenizer)\n",
    "    p, r = rouge_n(target, pred, 1)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "print(np.mean(precisions), np.mean(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(precisions), np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akilesh's Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "ix = 500\n",
    "print(np.array(test_df['target_text'])[ix])\n",
    "print(np.array(test_df[training_column])[ix])\n",
    "generate(np.array(test_df[training_column])[ix:ix+1], model, tokenizer, np.array(test_df['target_text'])[ix:ix+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "inp = np.array(test_df[training_column])[ix:ix+1]\n",
    "targ = np.array(test_df['target_text'])[ix:ix+1]\n",
    "# ai, target = generate(, model, tokenizer, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.append(test_df)\n",
    "for i in range(len(df['cat_conc_sec'])):\n",
    "    if 'Chief' in np.array(df['cat_conc_sec'])[i]:\n",
    "        print(np.array(df['cat_conc_sec'])[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Subjective:Action:Medications:go on | Subjective:Name:Medications:robitussin']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Plan:Timeframe:FollowUp:next week']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Subjective:Denies:Symptom:allergies']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Subjective:Complaint:ChiefComplaint:obesity']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "inp = ['Statistic Name: temperature | Statistic Value: 92']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Statistic Name: Blood Oxygen | Statistic Value: high']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "phaseIIoutput = [('go', 'Plan_MedicationsAction'), ('robitussin', 'Plan_MedicationsName'), ('adding', 'Plan_MedicationsAction'), ('go on', 'Plan_MedicationsAction')]\n",
    "\n",
    "inp = ['Action: go | Name: robitussin | Action: Adding | Action: go on']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "phaseIIoutput = [('go', 'Plan_MedicationsAction'), ('robitussin', 'Plan_MedicationsName'), ('adding', 'Plan_MedicationsAction'), ('go on', 'Plan_MedicationsAction')]\n",
    "\n",
    "inp = ['Location: lungs']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Name: constantly | Name: coughing | Frequency: every day']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Denies: allergies']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Denies: distress']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(\"AI told to write about: \", inp[0])\n",
    "print(\"AI Generated Sentence: \", ai[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\"))\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Action: on']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(\"AI told to write about: \", inp[0])\n",
    "print(\"AI Generated Sentence: \", ai[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\"))\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Type: xray | Location: right knee | Findings: fractured tibia']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(\"AI told to write about: \", inp[0])\n",
    "print(\"AI Generated Sentence: \", ai[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\"))\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Date: 1/12/1944 | Action: follow up']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(\"AI told to write about: \", inp[0])\n",
    "print(\"AI Generated Sentence: \", ai[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\"))\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
