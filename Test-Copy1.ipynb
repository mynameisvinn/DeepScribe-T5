{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration,Adafactor\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare train/test data\n",
    "create train and test from original.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 99)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('original.csv')\n",
    "df = test_df\n",
    "len(test_df), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = []\n",
    "for i in range(len(np.array(df['input_text']))):\n",
    "    if np.array(df['input_text'])[i] in np.array(test_df['input_text']):\n",
    "        mask.append(False)\n",
    "    else:\n",
    "        mask.append(True)\n",
    "mask = np.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df\n",
    "mask = np.random.rand(len(df)) < 0.8\n",
    "train_df = df[mask]\n",
    "test_df = df[~mask]\n",
    "train_df.to_csv('train_df.csv')\n",
    "test_df.to_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## figure out if there are long inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ars =  np.array(df['input_text'])\n",
    "mas = []\n",
    "for a in ars:\n",
    "    mas.append(len(a))\n",
    "mas = np.array(mas)\n",
    "len(np.where(mas>400)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('pytorch_model_categories.bin'))\n",
    "#model.load_state_dict(torch.load('pytorch_model_categories_87_precision.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "def progress(loss,value, max=100):\n",
    " return HTML(\"\"\" Batch loss :{loss}\n",
    "      <progress    \n",
    "value='{value}'max='{max}',style='width: 100%'>{value}\n",
    "      </progress>\n",
    "             \"\"\".format(loss=loss,value=value, max=max))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create test batches from test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_of_batches=len(test_df)/batch_size\n",
    "num_of_batches = int(num_of_batches)\n",
    "test_inp_batches = []\n",
    "test_label_batches =[]\n",
    "for i in range(num_of_batches):\n",
    "    \n",
    "    new_df=test_df[i*batch_size:i*batch_size+batch_size]\n",
    "    inputbatch=[]\n",
    "    labelbatch=[]\n",
    "    for indx,row in new_df.iterrows():\n",
    "          input = row[training_column]\n",
    "          labels = row['target_text'] \n",
    "          inputbatch.append(input)\n",
    "          labelbatch.append(labels)\n",
    "    test_inputbatch=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=400,return_tensors='pt')[\"input_ids\"]\n",
    "    test_labelbatch=tokenizer.batch_encode_plus(labelbatch,padding=True,max_length=400,return_tensors=\"pt\") [\"input_ids\"]\n",
    "    test_inputbatch=test_inputbatch.to(dev)\n",
    "    test_labelbatch=test_labelbatch.to(dev)\n",
    "    test_inp_batches.append(test_inputbatch)\n",
    "    test_label_batches.append(test_labelbatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision: when we predict the word and it's present in target set\n",
    "#Recall: When word is in target set and we predict it as well\n",
    "test_df.to_csv('test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'42_recall_51_lsa.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{precision, recall} = ', evaluate(np.array(train_df[training_column]), np.array(train_df['target_text']), model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df = df\n",
    "print('{precision, recall} = ', evaluate(np.array(test_df[training_column]), np.array(test_df['target_text']), model, tokenizer)), #0.0468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feed like this ? Incorporate medical concepts in the following way ?\n",
    "term 1 | term 2 |\n",
    "term 1 <concept1> | term 2 <concept2>\n",
    "\n",
    "instead of using terms, just use concepts ?\n",
    "\n",
    "train the model specifically for each category (e.g.: symptom, medications, family history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(test_df['input_text'])[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[15:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "ix = 500\n",
    "print(np.array(test_df['target_text'])[ix])\n",
    "print(np.array(test_df[training_column])[ix])\n",
    "generate(np.array(test_df[training_column])[ix:ix+1], model, tokenizer, np.array(test_df['target_text'])[ix:ix+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rouge modified evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = generate(np.array(test_df['input_text']), model, tokenizer, np.array(test_df['target_text']))\n",
    "pred_lsas = getlsa(preds)\n",
    "target_lsas = getlsa(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lsas.shape, len(preds), target_lsas.shape, len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9\n",
    "print(np.array(test_df['input_text'])[k], np.array(test_df['target_text'])[k], preds[k])\n",
    "sim(pred_lsas[k], target_lsas[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su = 0\n",
    "for k in range(len(preds)):\n",
    "    su += sim(pred_lsas[k], target_lsas[k])\n",
    "su, su/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions = []\n",
    "recalls = []\n",
    "preds, targets = generate(np.array(test_df['input_text']), model, tokenizer, np.array(test_df['target_text']))\n",
    "print('Predictions done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(targets)):\n",
    "    target = targets[i]\n",
    "    pred = preds[i]#generate(inp, model, tokenizer)\n",
    "    p, r = rouge_n(target, pred, 1)\n",
    "    precisions.append(p)\n",
    "    recalls.append(r)\n",
    "print(np.mean(precisions), np.mean(recalls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(precisions), np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Akilesh's Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "ix = 500\n",
    "print(np.array(test_df['target_text'])[ix])\n",
    "print(np.array(test_df[training_column])[ix])\n",
    "generate(np.array(test_df[training_column])[ix:ix+1], model, tokenizer, np.array(test_df['target_text'])[ix:ix+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "inp = np.array(test_df[training_column])[ix:ix+1]\n",
    "targ = np.array(test_df['target_text'])[ix:ix+1]\n",
    "# ai, target = generate(, model, tokenizer, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df.append(test_df)\n",
    "for i in range(len(df['cat_conc_sec'])):\n",
    "    if 'Chief' in np.array(df['cat_conc_sec'])[i]:\n",
    "        print(np.array(df['cat_conc_sec'])[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Subjective:Action:Medications:go on | Subjective:Name:Medications:robitussin']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Plan:Timeframe:FollowUp:next week']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Subjective:Denies:Symptom:allergies']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Subjective:Complaint:ChiefComplaint:obesity']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "inp = ['Statistic Name: temperature | Statistic Value: 92']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inp = ['Statistic Name: Blood Oxygen | Statistic Value: high']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "phaseIIoutput = [('go', 'Plan_MedicationsAction'), ('robitussin', 'Plan_MedicationsName'), ('adding', 'Plan_MedicationsAction'), ('go on', 'Plan_MedicationsAction')]\n",
    "\n",
    "inp = ['Action: go | Name: robitussin | Action: Adding | Action: go on']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "phaseIIoutput = [('go', 'Plan_MedicationsAction'), ('robitussin', 'Plan_MedicationsName'), ('adding', 'Plan_MedicationsAction'), ('go on', 'Plan_MedicationsAction')]\n",
    "\n",
    "inp = ['Location: lungs']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Name: constantly | Name: coughing | Frequency: every day']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Denies: allergies']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(ai)\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Denies: distress']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(\"AI told to write about: \", inp[0])\n",
    "print(\"AI Generated Sentence: \", ai[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\"))\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Action: on']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(\"AI told to write about: \", inp[0])\n",
    "print(\"AI Generated Sentence: \", ai[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\"))\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Type: xray | Location: right knee | Findings: fractured tibia']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(\"AI told to write about: \", inp[0])\n",
    "print(\"AI Generated Sentence: \", ai[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\"))\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "phaseIIouptut = [('constantly', 'Subjective_SymptomFrequency'), ('coughing', 'Subjective_SymptomName'), ('every day', 'Subjective_SymptomFrequency')]\n",
    "\n",
    "inp = ['Date: 1/12/1944 | Action: follow up']\n",
    "targ = ['Their temperature is 92.']\n",
    "ai, target = generate(np.array(inp), model, tokenizer, np.array(targ))\n",
    "print(\"AI told to write about: \", inp[0])\n",
    "print(\"AI Generated Sentence: \", ai[0].replace(\"<pad>\", \"\").replace(\"</s>\", \"\"))\n",
    "print(time.time() - start, \" seconds to generate this sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
